<!DOCTYPE HTML>
<html>
  <head>
    <meta http-equiv="Content-Type" context="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Mickey Chao - Projects</title>
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css">
    <link rel="stylesheet" type="text/css" media="screen" href="css/projects.css">

    <script src="https://www.w3schools.com/lib/w3.js"></script>
  </head>
  <body>
    <header>
      <div w3-include-html="nav.html"></div>
    </header>

    <section class="main animated zoomIn">
      <aside class="row1 col1">
        <div class="text-reuse">
          <h3>Text Reuse Detection</h3>
          <div class="content">
            <p>You've probably been tempted to copy someone's work while you
            were back in school, but you were deterred by the threat of
            plagiarism checkers! If you go back a few hundred years before the
            existence of computers and automated plagiarism checkers, how
            likely do you think it was that authors stole ideas from each other?
            Apparently there were a lot of dishonorable people!</p>
            <p>For this project at ProQuest, I created a machine learning
            pipeline to analyze hundreds of thousands of works of literature
            over several centuries and flag pairs of documents that contained
            similar passages. Furthermore, the pipeline identified the exact
            phrases that were too-similar.</p>
            <p>Some challenges that I addressed included
              <ol>
                <li>Having billions of pairs of documents that need to be
                  checked</li>
                <li>Accounting for variations of English ("tis ain't simple!")
                  </li>
                <li>Showing copied narrative structure in cases of major
                  plagiarism</li>
              </ol>
            </p>
            <p>This was a pretty successful project, though the results are
            not public. As a vague example, when I read the Wikipedia page
            praising an author for his brilliant works of literature, all I
            could say was "No duh. He just renamed all the characters from
            another reknowned author's work." These discoveries are expected
            to be useful for researchers at universities across the country.</p>
          </div>
        </div>
      </aside>
      <aside class="row1 col2">
        <div class="political-graphs">
          <h3>Analyzing Donor Graphs in US Politics</h3>
          <img src="images/political-graph-analysis.png"></img>
          <div class="content">
            <p>Suppose you hold liberal views and you vote for a representative
            with liberal views and she conveniently gets elected. You are happy
            that she votes on the bills you also support. You conclude that your
            representative is all-so-noble and works for you because you voted
            for her. The system works! ... Well can we definitely conclude that?
            </p>
            <p>Statistically speaking, all you can say is that your
            representative's political beliefs are highly-correlated to your
            district majority's beliefs. This is always the case, because the
            winner always has the majority of the votes. But you don't have
            proof, other than her word, that the way she acts in Congress is
            actually <i>caused</i> by the fact that she wanted your support. It
            could just be coincidence that your views and her secret agenda
            align.</p>
            <p>In this project, my group presented another variable that is
            highly-correlated to a legislator's political beliefs. We trained a
            neural network to help identify the political affiliation of a
            senator based on his/her corporate donors. We were able to separate
            Democrats from Republicans with an entropy score of ~0.2 (which is
            very accurate). Most of the mistakes were semi-liberal Republicans
            and semi-conservative Democrats. We were surprised to conclude
            that corporate donations are also highly correlated to senators'
            political affiliations (and also their importance in Congress!).
            </p>
            <p>Now you know that company donations are also just as good
            an indicator of political affiliation as your vote. So do you still
            have absolute faith in our system? Are you absolutely sure your
            representative works for your vote? Just stirring the mud here...
            don't take it too far. Our code was not that polished as we were
            trying to meet a class deadline but we have written up results in a
            very rough paper
              <a href="https://github.com/mjchao/Political-Graph-Analysis/blob/master/final_paper.pdf" 
                  target="_blank">here</a>.</p>
          </div>
        </div>
      </aside>
      <aside class="row1 col3">
        <div class="word2vec">
          <h3>Word Vectors for Shopping Ads</h3>
          <img src="images/word2vec.png"></img>
          <div class="content">
            <p>If we want a computer to recommend products for a customer to buy
            given his/her query, the computer needs to understand the
            similarities between words. That way, it can match descriptions of
            products to the customer's query.</p>
            <p>Unfortunately, when you give words to a computer, it doesn't
            really have a great way of determining how similar they are. Words
            on a computer are just a bunch of characters and the individual
            characters don't tell much about the meaning of the word. But we
            can try representing words with something that better expresses
            similarity: vectors.</p>
            <p>For this project, my goal was to represent words in Google's
            product data with vectors to express similarity with words
            that might also be in customer queries. We assume that similar words
            are used in similar contexts. Then, the general idea is to learn
            a set of vectors to represent words and a neural network to
            take a word's vector representation and predict its likely context
            words. For example, in the phrase "the cat sat," we'd
            want the network to predict "the" and "sat" when given the word
            vector for "cat". The training data consisted of billions of
            (target, context) pairs of words built from a database of product
            data.</p>
            <p>After these word vectors were learned, we could see, for example,
            that words like "king" and "queen" were closely related to
            "mattress". These vectors were then used as pre-trained data for a
            sophisticated pipeline that combined features of paragraphs and
            images in product data to recommend to shoppers.</p>
          </div>
        </div>
      </aside>
      <aside class="row2 col1">
        <div class="food-sharing-backend">
          <h3>Food Sharing App Backend</h3>

          <div class="content">

          </div>
        </div>
      </aside>
      <aside class="row2 col2">
        <div class="gateway-test-framework">
          <h3>Gateway Test Framework</h3>
          <img src="images/gateway-test-framework.png"></img>
          <div class="content">

          </div>
        <div>
      </aside>
      <aside class="row2 col3">
        <div class="graph3d">
          <h3>3D Graphing Calculator</h3>
          <img src="images/graph3d.png"></img>
          <div class="content">

          </div>
        </div>
      </aside>
    </section>
  </body>

  <script>w3.includeHTML();</script>
  <script src="js/gridlayout.js"></script>
</html>

