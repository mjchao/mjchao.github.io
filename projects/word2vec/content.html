<p>
If we want a computer to recommend products given a customer’s query, the computer needs to understand the similarities between words. That way, it can match descriptions of products to the query.
</p>
<p>
Unfortunately, when you give words to a computer, it doesn’t really have a great way of determining how similar they are. Words on a computer are just a bunch of characters and the individual characters don’t tell much about the meaning of the word. But we can try representing words with something that better expresses similarity: vectors.
</p>
<p>
First, I operated under the assumption that similar words are used in similar contexts. For example, “I own a Lenovo Thinkpad” and “I own a Lenovo computer” have very similar meanings. We see that the words around “Thinkpad” and “computer” are the same. Therefore, it would make sense that “Thinkpad” and “computer” are very similar words. This observation is known as the distributional hypothesis.
</p>
<p>
To capitalize on the distributional hypothesis, we want to learn word vectors and a model (typically a neural network) that takes in a word vector and predicts the word vector’s context words with high probability. If the model received the word vector for “computer,” it should predict “I”, “own”, “a”, and “Lenovo” with high probability. If the model received the word vector for “Thinkpad”, the model should do the same as well. Under these constraints, the word vector for “Thinkpad” will grow close to the word vector for “computer” as the model trains.
</p>
<p>
After these word vectors were learned, we could see, for example, that words like “lenovo” and “computer” were closely related to each other and to “cpu.” These vectors were then used as pre-trained data for a sophisticated pipeline that combined features of paragraphs and images in product data to generate recommendations for shoppers.
</p>
